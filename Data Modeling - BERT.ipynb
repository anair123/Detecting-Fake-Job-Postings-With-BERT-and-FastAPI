{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1d0bb-7f01-4830-be62-82d3198e5c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade s3fs\n",
    "!pip uninstall botocore -y\n",
    "!pip install botocore\n",
    "!pip install --upgrade boto3\n",
    "!pip install tensorflow-hub\n",
    "!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac515a-5598-45c4-a545-45b5854bd31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740c8788-59e3-4fab-8b03-362d40683a09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 01:44:08.326208: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-11 01:44:08.326249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-11 01:44:08.327392: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import s3fs\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig, DistilBertTokenizerFast\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tempfile\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38616a1-5d52-4d30-b3f5-7144f0d12686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c86a24-1049-4a9c-bb22-ae570ab6d399",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97f693c-f484-46c5-8369-e5dda8e1bd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export these datasets to s3 as csv files\n",
    "s3_path = 's3://fake-job-posting-detection/Data'\n",
    "train = pd.read_csv(f'{s3_path}/train.csv')\n",
    "train_2x = pd.read_csv(f'{s3_path}/augmented_train_2x.csv')\n",
    "train_10x = pd.read_csv(f'{s3_path}/augmented_train_10x.csv')\n",
    "test = pd.read_csv(f'{s3_path}/test.csv')\n",
    "validation = pd.read_csv(f'{s3_path}/validation.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec04711-bda5-4222-971a-a5221a931e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare input and output data\n",
    "X_train = train['text']\n",
    "y_train = train['fraudulent']\n",
    "X_train_2x = train_2x['text']\n",
    "y_train_2x = train_2x['fraudulent']\n",
    "X_train_10x = train_10x['text']\n",
    "y_train_10x = train_10x['fraudulent']\n",
    "X_valid = validation['text']\n",
    "y_valid = validation['fraudulent']\n",
    "X_test = test['text']\n",
    "y_test = test['fraudulent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1b855-34af-4959-a1b0-55c3ed126248",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "767f0d78-1164-4b09-b500-21befcfb9a45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04efa3265c64894b62ff6f34ddbc131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bace66f0976d45d8939e13b57deacb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b275a5addb4e0e867b91765c74baf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6a4bfa2c084bf2ae2b31016b5607d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "max_length=512\n",
    "\n",
    "# Tokenize the training, validation, and testing input data\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "train_2x_encodings = tokenizer(X_train_2x.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "train_10x_encodings = tokenizer(X_train_10x.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(X_valid.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert target labels to TensorFlow tensors\n",
    "train_labels = tf.convert_to_tensor(y_train)\n",
    "train_labels_2x = tf.convert_to_tensor(y_train_2x)\n",
    "train_labels_10x = tf.convert_to_tensor(y_train_10x)\n",
    "val_labels = tf.convert_to_tensor(y_valid)\n",
    "test_labels = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d2c785-1db4-4249-84e7-9cdba52366e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d9d8f1-a299-44cd-b9f7-4b557bcdbe14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6561711f02e44a18b8a9dfb958d10a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122534497abb4090ae72bce8ca5f1aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109482240 (417.64 MB)\n",
      "Trainable params: 109482240 (417.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)      TFBaseModelOutputWithPooli   1094822   ['input_ids[0][0]',           \n",
      "                             ngAndCrossAttentions(last_   40         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 512, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  196864    ['bert[0][1]']                \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 256)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " outputs (Dense)             (None, 1)                    257       ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109679361 (418.39 MB)\n",
      "Trainable params: 197121 (770.00 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    # initialize the BERT model\n",
    "    bert = TFAutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # two input layers, layer name variables MUST match the dictionary keys \n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    # we access the transformer model within our bert object using the bert attribute \n",
    "    embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access final activations (alread max-pooled) [1]\n",
    "    \n",
    "    \n",
    "    # convert bert embeddings into 2 output classes\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(embeddings)\n",
    "\n",
    "    # Add a dropout layer\n",
    "    dropout_rate = 0.3\n",
    "    x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # add the output layer\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(x)\n",
    "\n",
    "    # initialize model\n",
    "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "    # freeze bert layer\n",
    "    model.layers[2].trainable = False\n",
    "\n",
    "    # print out model summary\n",
    "    return model\n",
    "\n",
    "# create a model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e6e3d4e-7e58-4cb5-81d3-42d6d221c296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "\n",
    "# Assuming train_encodings and val_encodings are BatchEncoding objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    train_labels  # Assuming train_labels is your training labels\n",
    ")).shuffle(100).batch(batch_size)\n",
    "\n",
    "# Assuming train_encodings and val_encodings are BatchEncoding objects\n",
    "train_dataset_2x = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: train_2x_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    train_labels_2x  # Assuming train_labels is your training labels\n",
    ")).shuffle(100).batch(batch_size)\n",
    "\n",
    "\n",
    "# Assuming train_encodings and val_encodings are BatchEncoding objects\n",
    "train_dataset_10x = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: train_10x_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    train_labels_10x  # Assuming train_labels is your training labels\n",
    ")).shuffle(100).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    val_labels  # Assuming val_labels is your validation labels\n",
    ")).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: test_encodings[key] for key in ['input_ids', 'attention_mask']},\n",
    "    test_labels  # Assuming val_labels is your validation labels\n",
    ")).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9a445-af6c-4920-a895-51c3e8b28d44",
   "metadata": {},
   "source": [
    "### Training Without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4108b09-eebd-49c1-af32-111f113c5e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_epochs=30\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5, decay=1e-6)\n",
    "loss = 'binary_crossentropy'  # Use binary crossentropy for binary classification\n",
    "auc = tf.keras.metrics.AUC()  # Use AUC as the metric for binary classification\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[auc])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Then use these datasets to train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4096824-5542-4fb8-a7c9-2580f5e0384b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 117s 5s/step - loss: 0.1659 - auc: 0.8001\n"
     ]
    }
   ],
   "source": [
    "# evluate the dataset\n",
    "results = model.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a7a27e-3d52-4de8-a1ad-b138ce37183e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.165871262550354\n",
      "Test AUC: 0.8001484274864197\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation results\n",
    "print(\"Test Loss:\", results[0])  # Prints the test loss\n",
    "print(\"Test AUC:\", results[1])  # Prints the test accuracy (if accuracy was specified as a metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80903c84-daf3-4cb8-9acb-b45f9fc28722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# set path for model\n",
    "s3_bucket_name = 'fake-job-posting-detection'\n",
    "s3_key = 'Model/model.h5'\n",
    "\n",
    "# Save the model to a temporary file locally\n",
    "with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:\n",
    "    model.save(temp_file.name)\n",
    "\n",
    "# Create a connection to S3 using boto3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the temporary model file to S3\n",
    "s3_client.upload_file(temp_file.name, s3_bucket_name, s3_key)\n",
    "\n",
    "# Remove the temporary file after upload\n",
    "temp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2348c69-b262-4d7b-82a4-8a5251348ae9",
   "metadata": {},
   "source": [
    "### Training with data augmentation (2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a9369fb-ecdd-4fd3-86ba-4e88523f0f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109482240 (417.64 MB)\n",
      "Trainable params: 109482240 (417.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2x = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a5947707-45c3-4173-8eee-6b603e58d1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "44/44 [==============================] - 320s 7s/step - loss: 0.4214 - auc_2: 0.4026 - val_loss: 0.2017 - val_auc_2: 0.4945\n",
      "Epoch 2/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3448 - auc_2: 0.4312 - val_loss: 0.1915 - val_auc_2: 0.5298\n",
      "Epoch 3/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3472 - auc_2: 0.4009 - val_loss: 0.1884 - val_auc_2: 0.5809\n",
      "Epoch 4/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3366 - auc_2: 0.4419 - val_loss: 0.1873 - val_auc_2: 0.6059\n",
      "Epoch 5/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3416 - auc_2: 0.4071 - val_loss: 0.1864 - val_auc_2: 0.6184\n",
      "Epoch 6/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3314 - auc_2: 0.4495 - val_loss: 0.1858 - val_auc_2: 0.6521\n",
      "Epoch 7/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3295 - auc_2: 0.4619 - val_loss: 0.1857 - val_auc_2: 0.6522\n",
      "Epoch 8/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3315 - auc_2: 0.4502 - val_loss: 0.1848 - val_auc_2: 0.6679\n",
      "Epoch 9/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3263 - auc_2: 0.4760 - val_loss: 0.1849 - val_auc_2: 0.6794\n",
      "Epoch 10/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3256 - auc_2: 0.4752 - val_loss: 0.1845 - val_auc_2: 0.6828\n",
      "Epoch 11/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3236 - auc_2: 0.4814 - val_loss: 0.1845 - val_auc_2: 0.6855\n",
      "Epoch 12/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3195 - auc_2: 0.4977 - val_loss: 0.1845 - val_auc_2: 0.6856\n",
      "Epoch 13/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3220 - auc_2: 0.4933 - val_loss: 0.1844 - val_auc_2: 0.6866\n",
      "Epoch 14/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3200 - auc_2: 0.4925 - val_loss: 0.1847 - val_auc_2: 0.6817\n",
      "Epoch 15/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3180 - auc_2: 0.5066 - val_loss: 0.1843 - val_auc_2: 0.6809\n",
      "Epoch 16/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3150 - auc_2: 0.5266 - val_loss: 0.1838 - val_auc_2: 0.6859\n",
      "Epoch 17/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3097 - auc_2: 0.5492 - val_loss: 0.1839 - val_auc_2: 0.6884\n",
      "Epoch 18/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3120 - auc_2: 0.5355 - val_loss: 0.1837 - val_auc_2: 0.6889\n",
      "Epoch 19/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3130 - auc_2: 0.5319 - val_loss: 0.1834 - val_auc_2: 0.6898\n",
      "Epoch 20/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3043 - auc_2: 0.5764 - val_loss: 0.1836 - val_auc_2: 0.6861\n",
      "Epoch 21/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3081 - auc_2: 0.5559 - val_loss: 0.1832 - val_auc_2: 0.6898\n",
      "Epoch 22/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3021 - auc_2: 0.5950 - val_loss: 0.1831 - val_auc_2: 0.6881\n",
      "Epoch 23/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3053 - auc_2: 0.5780 - val_loss: 0.1829 - val_auc_2: 0.6919\n",
      "Epoch 24/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3037 - auc_2: 0.5844 - val_loss: 0.1830 - val_auc_2: 0.6897\n",
      "Epoch 25/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3043 - auc_2: 0.5831 - val_loss: 0.1825 - val_auc_2: 0.6898\n",
      "Epoch 26/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.2962 - auc_2: 0.6225 - val_loss: 0.1828 - val_auc_2: 0.6904\n",
      "Epoch 27/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3007 - auc_2: 0.6016 - val_loss: 0.1821 - val_auc_2: 0.6984\n",
      "Epoch 28/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.2982 - auc_2: 0.6119 - val_loss: 0.1822 - val_auc_2: 0.6935\n",
      "Epoch 29/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.2998 - auc_2: 0.6048 - val_loss: 0.1819 - val_auc_2: 0.7014\n",
      "Epoch 30/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3016 - auc_2: 0.5923 - val_loss: 0.1818 - val_auc_2: 0.7019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fdb84b374f0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs=30\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5, decay=1e-6)\n",
    "loss = 'binary_crossentropy'  # Use binary crossentropy for binary classification\n",
    "auc = tf.keras.metrics.AUC()  # Use AUC as the metric for binary classification\n",
    "\n",
    "# compile the model\n",
    "model_2x.compile(optimizer=optimizer, loss=loss, metrics=[auc])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Then use these datasets to train the model\n",
    "model_2x.fit(train_dataset_2x, validation_data=val_dataset, epochs=num_epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8cf4ac03-e0f2-4a55-a8d2-e69ac050e8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s3_bucket_name = 'fake-job-posting-detection'\n",
    "s3_key = 'Model/model_2x_augmentation.h5'\n",
    "\n",
    "# Save the model to a temporary file locally\n",
    "with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:\n",
    "    model_2x.save(temp_file.name)\n",
    "\n",
    "# Create a connection to S3 using boto3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the temporary model file to S3\n",
    "s3_client.upload_file(temp_file.name, s3_bucket_name, s3_key)\n",
    "\n",
    "# Remove the temporary file after upload\n",
    "temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7839d-1b07-4168-8923-cbb839fa564d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f01ee7-4657-40c0-985c-bcd8ac3eabb6",
   "metadata": {},
   "source": [
    "### Training with data augmentation (10x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a88b48f9-4347-4130-9168-e464ea398aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109482240 (417.64 MB)\n",
      "Trainable params: 109482240 (417.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_10x = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "68ca42c8-0ccb-4e32-8590-4eb15058a436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "44/44 [==============================] - 320s 7s/step - loss: 0.4118 - auc_3: 0.3965 - val_loss: 0.1977 - val_auc_3: 0.4631\n",
      "Epoch 2/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3370 - auc_3: 0.4121 - val_loss: 0.1924 - val_auc_3: 0.4852\n",
      "Epoch 3/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3402 - auc_3: 0.3873 - val_loss: 0.1889 - val_auc_3: 0.5535\n",
      "Epoch 4/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3341 - auc_3: 0.4104 - val_loss: 0.1886 - val_auc_3: 0.6104\n",
      "Epoch 5/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3331 - auc_3: 0.4166 - val_loss: 0.1879 - val_auc_3: 0.6222\n",
      "Epoch 6/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3307 - auc_3: 0.4256 - val_loss: 0.1877 - val_auc_3: 0.6288\n",
      "Epoch 7/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3282 - auc_3: 0.4356 - val_loss: 0.1874 - val_auc_3: 0.6367\n",
      "Epoch 8/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3258 - auc_3: 0.4473 - val_loss: 0.1873 - val_auc_3: 0.6353\n",
      "Epoch 9/30\n",
      "44/44 [==============================] - 306s 7s/step - loss: 0.3255 - auc_3: 0.4444 - val_loss: 0.1874 - val_auc_3: 0.6319\n",
      "Epoch 10/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3197 - auc_3: 0.4765 - val_loss: 0.1872 - val_auc_3: 0.6443\n",
      "Epoch 11/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3194 - auc_3: 0.4840 - val_loss: 0.1872 - val_auc_3: 0.6436\n",
      "Epoch 12/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3199 - auc_3: 0.4797 - val_loss: 0.1870 - val_auc_3: 0.6494\n",
      "Epoch 13/30\n",
      "44/44 [==============================] - 306s 7s/step - loss: 0.3158 - auc_3: 0.5015 - val_loss: 0.1868 - val_auc_3: 0.6437\n",
      "Epoch 14/30\n",
      "44/44 [==============================] - 306s 7s/step - loss: 0.3149 - auc_3: 0.5075 - val_loss: 0.1867 - val_auc_3: 0.6485\n",
      "Epoch 15/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3140 - auc_3: 0.5110 - val_loss: 0.1864 - val_auc_3: 0.6529\n",
      "Epoch 16/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3115 - auc_3: 0.5218 - val_loss: 0.1866 - val_auc_3: 0.6510\n",
      "Epoch 17/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3107 - auc_3: 0.5308 - val_loss: 0.1862 - val_auc_3: 0.6566\n",
      "Epoch 18/30\n",
      "44/44 [==============================] - 308s 7s/step - loss: 0.3106 - auc_3: 0.5322 - val_loss: 0.1861 - val_auc_3: 0.6632\n",
      "Epoch 19/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3085 - auc_3: 0.5479 - val_loss: 0.1859 - val_auc_3: 0.6625\n",
      "Epoch 20/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3088 - auc_3: 0.5380 - val_loss: 0.1858 - val_auc_3: 0.6582\n",
      "Epoch 21/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3053 - auc_3: 0.5631 - val_loss: 0.1856 - val_auc_3: 0.6660\n",
      "Epoch 22/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3039 - auc_3: 0.5725 - val_loss: 0.1856 - val_auc_3: 0.6583\n",
      "Epoch 23/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3033 - auc_3: 0.5768 - val_loss: 0.1851 - val_auc_3: 0.6671\n",
      "Epoch 24/30\n",
      "44/44 [==============================] - 306s 7s/step - loss: 0.3019 - auc_3: 0.5830 - val_loss: 0.1853 - val_auc_3: 0.6677\n",
      "Epoch 25/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.2999 - auc_3: 0.5940 - val_loss: 0.1848 - val_auc_3: 0.6685\n",
      "Epoch 26/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.3002 - auc_3: 0.5916 - val_loss: 0.1846 - val_auc_3: 0.6709\n",
      "Epoch 27/30\n",
      "44/44 [==============================] - 306s 7s/step - loss: 0.3007 - auc_3: 0.5963 - val_loss: 0.1846 - val_auc_3: 0.6727\n",
      "Epoch 28/30\n",
      "44/44 [==============================] - 306s 7s/step - loss: 0.2984 - auc_3: 0.6087 - val_loss: 0.1841 - val_auc_3: 0.6797\n",
      "Epoch 29/30\n",
      "44/44 [==============================] - 306s 7s/step - loss: 0.2976 - auc_3: 0.6145 - val_loss: 0.1839 - val_auc_3: 0.6808\n",
      "Epoch 30/30\n",
      "44/44 [==============================] - 307s 7s/step - loss: 0.2939 - auc_3: 0.6312 - val_loss: 0.1836 - val_auc_3: 0.6772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fdb541250f0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs=30\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5, decay=1e-6)\n",
    "loss = 'binary_crossentropy'  # Use binary crossentropy for binary classification\n",
    "auc = tf.keras.metrics.AUC()  # Use AUC as the metric for binary classification\n",
    "\n",
    "# compile the model\n",
    "model_10x.compile(optimizer=optimizer, loss=loss, metrics=[auc])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Then use these datasets to train the model\n",
    "model_10x.fit(train_dataset_10x, validation_data=val_dataset, epochs=num_epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc97849d-83c1-4463-bbc4-7ee320cbd5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file path\n",
    "s3_bucket_name = 'fake-job-posting-detection'\n",
    "s3_key = 'Model/model_10x_augmentation.h5'\n",
    "\n",
    "# Save the model to a temporary file locally\n",
    "with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:\n",
    "    model_10x.save(temp_file.name)\n",
    "\n",
    "# Create a connection to S3 using boto3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Upload the temporary model file to S3\n",
    "s3_client.upload_file(temp_file.name, s3_bucket_name, s3_key)\n",
    "\n",
    "# Remove the temporary file after upload\n",
    "temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac9f9d-64e8-4a22-9e93-4be030fdcae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7295521-efc3-4496-922e-ec13e26f14b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
